{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g2dCFLWT31G2"
   },
   "source": [
    "## Import all packages and functions needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PGevpW6_3k54"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z73zUN8O0K6j"
   },
   "source": [
    "## Decision tree class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dyk6Mbvkysc7"
   },
   "outputs": [],
   "source": [
    "class dtree:\n",
    "    \"\"\" A basic Decision Tree\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "\n",
    "    def read_data(self, filename):\n",
    "        fid = open(filename, \"r\")\n",
    "        data = []\n",
    "        d = []\n",
    "        for line in fid.readlines():\n",
    "            d.append(line.strip())\n",
    "        for d1 in d:\n",
    "            data.append(d1.split(\",\"))\n",
    "        fid.close()\n",
    "\n",
    "        self.featureNames = data[0]\n",
    "        self.featureNames = self.featureNames[:-1]\n",
    "        data = data[1:]\n",
    "        self.classes = []\n",
    "        for d in range(len(data)):\n",
    "            self.classes.append(data[d][-1])\n",
    "            data[d] = data[d][:-1]\n",
    "\n",
    "        return data, self.classes, self.featureNames\n",
    "\n",
    "    def classify(self, tree, datapoint):\n",
    "\n",
    "        if type(tree) == type(\"string\"):\n",
    "            # Have reached a leaf\n",
    "            return tree\n",
    "        else:\n",
    "            a = list(tree.keys())[0]\n",
    "            for i in range(len(self.featureNames)):\n",
    "                if self.featureNames[i] == a:\n",
    "                    break\n",
    "            try:\n",
    "                t = tree[a][datapoint[i]]\n",
    "                return self.classify(t, datapoint)\n",
    "            except:\n",
    "                return None\n",
    "\n",
    "    def classifyAll(self, tree, data):\n",
    "        results = []\n",
    "        for i in range(len(data)):\n",
    "            results.append(self.classify(tree, data[i]))\n",
    "        return results\n",
    "\n",
    "    def make_tree(self,\n",
    "                  data,\n",
    "                  classes,\n",
    "                  featureNames,\n",
    "                  maxlevel=-1,\n",
    "                  level=0,\n",
    "                  forest=0):\n",
    "        \"\"\" The main function, which recursively constructs the tree\"\"\"\n",
    "\n",
    "        nData = len(data)\n",
    "        nFeatures = len(data[0])\n",
    "\n",
    "        try:\n",
    "            self.featureNames\n",
    "        except:\n",
    "            self.featureNames = featureNames\n",
    "\n",
    "        # List the possible classes\n",
    "        newClasses = []\n",
    "        for aclass in classes:\n",
    "            if newClasses.count(aclass) == 0:\n",
    "                newClasses.append(aclass)\n",
    "\n",
    "        # Compute the default class (and total entropy)\n",
    "        frequency = np.zeros(len(newClasses))\n",
    "\n",
    "        totalEntropy = 0\n",
    "        totalGini = 0\n",
    "        index = 0\n",
    "        for aclass in newClasses:\n",
    "            frequency[index] = classes.count(aclass)\n",
    "            totalEntropy += self.calc_entropy(float(frequency[index]) / nData)\n",
    "            totalGini += (float(frequency[index]) / nData)**2\n",
    "\n",
    "            index += 1\n",
    "\n",
    "        totalGini = 1 - totalGini\n",
    "        default = classes[np.argmax(frequency)]\n",
    "\n",
    "        if nData == 0 or nFeatures == 0 or (maxlevel >= 0\n",
    "                                            and level > maxlevel):\n",
    "            # Have reached an empty branch\n",
    "            return default\n",
    "        elif classes.count(classes[0]) == nData:\n",
    "            # Only 1 class remains\n",
    "            return classes[0]\n",
    "        else:\n",
    "\n",
    "            # Choose which feature is best\n",
    "            gain = np.zeros(nFeatures)\n",
    "            ggain = np.zeros(nFeatures)\n",
    "            featureSet = list(range(nFeatures))\n",
    "            if forest != 0:\n",
    "                np.random.shuffle(featureSet)\n",
    "                featureSet = featureSet[0:forest]\n",
    "            for feature in featureSet:\n",
    "                g, gg = self.calc_info_gain(data, classes, feature)\n",
    "                gain[feature] = totalEntropy - g\n",
    "                ggain[feature] = totalGini - gg\n",
    "\n",
    "            #changed this\n",
    "            bestFeature = np.argmax(ggain)\n",
    "            \n",
    "            tree = {featureNames[bestFeature]: {}}\n",
    "\n",
    "            # List the values that bestFeature can take\n",
    "            values = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] not in values:\n",
    "                    values.append(datapoint[bestFeature])\n",
    "\n",
    "            for value in values:\n",
    "                # Find the datapoints with each feature value\n",
    "                newData = []\n",
    "                newClasses = []\n",
    "                index = 0\n",
    "                for datapoint in data:\n",
    "                    if datapoint[bestFeature] == value:\n",
    "                        if bestFeature == 0:\n",
    "                            newdatapoint = datapoint[1:]\n",
    "                            newNames = featureNames[1:]\n",
    "                        elif bestFeature == nFeatures:\n",
    "                            newdatapoint = datapoint[:-1]\n",
    "                            newNames = featureNames[:-1]\n",
    "                        else:\n",
    "                            newdatapoint = datapoint[:bestFeature]\n",
    "                            # newdatapoint.append(datapoint[bestFeature+1:])\n",
    "                            newdatapoint = np.append(\n",
    "                                newdatapoint, datapoint[bestFeature + 1:])\n",
    "                            newNames = featureNames[:bestFeature]\n",
    "                            # newNames.append(featureNames[bestFeature+1:])\n",
    "                            newNames = np.append(\n",
    "                                newNames, featureNames[bestFeature + 1:])\n",
    "                        newData.append(newdatapoint)\n",
    "                        newClasses.append(classes[index])\n",
    "                    index += 1\n",
    "\n",
    "                # Now recurse to the next level\n",
    "                subtree = self.make_tree(newData, newClasses, newNames,\n",
    "                                         maxlevel, level + 1, forest)\n",
    "\n",
    "                # And on returning, add the subtree on to the tree\n",
    "                tree[featureNames[bestFeature]][value] = subtree\n",
    "\n",
    "            return tree\n",
    "\n",
    "    def printTree(self, tree, name):\n",
    "        if type(tree) == dict:\n",
    "            print(name, tree.keys()[0])\n",
    "            for item in tree.values()[0].keys():\n",
    "                print(name, item)\n",
    "                self.printTree(tree.values()[0][item], name + \"\\t\")\n",
    "        else:\n",
    "            print(name, \"\\t->\\t\", tree)\n",
    "\n",
    "    def calc_entropy(self, p):\n",
    "        if p != 0:\n",
    "            return -p * np.log2(p)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def calc_info_gain(self, data, classes, feature):\n",
    "\n",
    "        # Calculates the information gain based on both entropy and the Gini impurity\n",
    "        gain = 0\n",
    "        ggain = 0\n",
    "        nData = len(data)\n",
    "\n",
    "        # List the values that feature can take\n",
    "        values = []\n",
    "        for datapoint in data:\n",
    "            if datapoint[feature] not in values:\n",
    "                values.append(datapoint[feature])\n",
    "\n",
    "        featureCounts = np.zeros(len(values))\n",
    "        entropy = np.zeros(len(values))\n",
    "        gini = np.zeros(len(values))\n",
    "        valueIndex = 0\n",
    "        \n",
    "        # Find where those values appear in data[feature] and the corresponding class\n",
    "        for value in values:\n",
    "            dataIndex = 0\n",
    "            newClasses = []\n",
    "            for datapoint in data:\n",
    "                if datapoint[feature] == value:\n",
    "                    featureCounts[valueIndex] += 1\n",
    "                    newClasses.append(classes[dataIndex])\n",
    "                dataIndex += 1\n",
    "\n",
    "            # Get the values in newClasses\n",
    "            classValues = []\n",
    "            for aclass in newClasses:\n",
    "                if classValues.count(aclass) == 0:\n",
    "                    classValues.append(aclass)\n",
    "\n",
    "            classCounts = np.zeros(len(classValues))\n",
    "            classIndex = 0\n",
    "            for classValue in classValues:\n",
    "                for aclass in newClasses:\n",
    "                    if aclass == classValue:\n",
    "                        classCounts[classIndex] += 1\n",
    "                classIndex += 1\n",
    "\n",
    "            for classIndex in range(len(classValues)):\n",
    "                entropy[valueIndex] += self.calc_entropy(\n",
    "                    float(classCounts[classIndex]) / np.sum(classCounts))\n",
    "                gini[valueIndex] += (\n",
    "                    float(classCounts[classIndex]) / np.sum(classCounts))**2\n",
    "\n",
    "            # Computes both the Gini gain and the entropy\n",
    "            gain = gain + float(\n",
    "                featureCounts[valueIndex]) / nData * entropy[valueIndex]\n",
    "            ggain = ggain + float(\n",
    "                featureCounts[valueIndex]) / nData * gini[valueIndex]\n",
    "            valueIndex += 1\n",
    "        print(gain)\n",
    "        return gain, 1 - ggain\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CwD3SlLJyuSA"
   },
   "source": [
    "## Testing decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1140,
     "status": "ok",
     "timestamp": 1554517373397,
     "user": {
      "displayName": "Chao Tao",
      "photoUrl": "https://lh5.googleusercontent.com/-jFo-3ih4A9w/AAAAAAAAAAI/AAAAAAAAABs/gNwz73yXKSs/s64/photo.jpg",
      "userId": "14116835992842562865"
     },
     "user_tz": 240
    },
    "id": "sAJKENqOypjU",
    "outputId": "e3b41dea-82ea-4201-ae38-6bf43200aa6a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'x1': {0: {'x2': {0: 0, 1: 1}}, 1: 1}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[0, 0], [1, 0], [0, 1], [1, 1]]\n",
    "classes = [0, 1, 1, 1]\n",
    "names = ['x1', 'x2']\n",
    "tree = dtree().make_tree(data, classes, names)\n",
    "tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the implementation was given to us, I didn't make many changes to the code. I changed the best feature value to consider the Gini gain, rather than the entropy. This sped the overall function up by allowing it to be more efficent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ddLMnZ5h0S_z"
   },
   "source": [
    "## Random forest class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uiq7nk3oyR0o"
   },
   "outputs": [],
   "source": [
    "class randomforest:\n",
    "    \"\"\"The random forest algorithm based on the decision tree of Chapter 6\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        self.tree = dtree()\n",
    "\n",
    "    def rf(self,\n",
    "           data,\n",
    "           targets,\n",
    "           features,\n",
    "           nTrees,     #Change this\n",
    "           nSamples,\n",
    "           nFeatures,  #Change this\n",
    "           maxlevel=5):\n",
    "\n",
    "        nPoints = np.shape(data)[0]\n",
    "        nDim = np.shape(data)[1]\n",
    "        self.nSamples = nSamples\n",
    "        self.nTrees = nTrees\n",
    "\n",
    "        classifiers = []\n",
    "\n",
    "        for i in range(nTrees):\n",
    "            print(i)\n",
    "            # Compute bootstrap samples\n",
    "            samplePoints = np.random.randint(0, nPoints, (nPoints, nSamples))\n",
    "\n",
    "            for j in range(nSamples):\n",
    "                sample = []\n",
    "                sampleTarget = []\n",
    "                for k in range(nPoints):\n",
    "                    sample.append(data[samplePoints[k, j], :])\n",
    "                    sampleTarget.append(targets[samplePoints[k, j]])\n",
    "            # Train classifiers\n",
    "            classifiers.append(\n",
    "                self.tree.make_tree(\n",
    "                    sample, sampleTarget, features, maxlevel,\n",
    "                    forest=nFeatures))\n",
    "        return classifiers\n",
    "\n",
    "    def rfclass(self, classifiers, data):\n",
    "\n",
    "        decision = []\n",
    "        # Majority voting\n",
    "        for j in range(len(data)):\n",
    "            outputs = []\n",
    "            #print data[j]\n",
    "            for i in range(self.nTrees):\n",
    "                out = self.tree.classify(classifiers[i], data[j, :])\n",
    "                if out is not None:\n",
    "                    outputs.append(out)\n",
    "            # List the possible outputs\n",
    "            out = []\n",
    "            for each in outputs:\n",
    "                if out.count(each) == 0:\n",
    "                    out.append(each)\n",
    "            frequency = np.zeros(len(out))\n",
    "\n",
    "            index = 0\n",
    "            if len(out) > 0:\n",
    "                for each in out:\n",
    "                    frequency[index] = outputs.count(each)\n",
    "                    index += 1\n",
    "                decision.append(out[frequency.argmax()])\n",
    "            else:\n",
    "                decision.append(None)\n",
    "        return decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LxlFNujcdFPB"
   },
   "source": [
    "## Testing random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 405
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1486,
     "status": "ok",
     "timestamp": 1554517373764,
     "user": {
      "displayName": "Chao Tao",
      "photoUrl": "https://lh5.googleusercontent.com/-jFo-3ih4A9w/AAAAAAAAAAI/AAAAAAAAABs/gNwz73yXKSs/s64/photo.jpg",
      "userId": "14116835992842562865"
     },
     "user_tz": 240
    },
    "id": "K7DWzsTxQkRB",
    "outputId": "96f21faa-2a2f-4eb9-9e3d-0988cc376ba9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>843786</td>\n",
       "      <td>M</td>\n",
       "      <td>12.45</td>\n",
       "      <td>15.70</td>\n",
       "      <td>82.57</td>\n",
       "      <td>477.1</td>\n",
       "      <td>0.12780</td>\n",
       "      <td>0.17000</td>\n",
       "      <td>0.15780</td>\n",
       "      <td>0.08089</td>\n",
       "      <td>...</td>\n",
       "      <td>15.47</td>\n",
       "      <td>23.75</td>\n",
       "      <td>103.40</td>\n",
       "      <td>741.6</td>\n",
       "      <td>0.1791</td>\n",
       "      <td>0.5249</td>\n",
       "      <td>0.5355</td>\n",
       "      <td>0.1741</td>\n",
       "      <td>0.3985</td>\n",
       "      <td>0.12440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>844359</td>\n",
       "      <td>M</td>\n",
       "      <td>18.25</td>\n",
       "      <td>19.98</td>\n",
       "      <td>119.60</td>\n",
       "      <td>1040.0</td>\n",
       "      <td>0.09463</td>\n",
       "      <td>0.10900</td>\n",
       "      <td>0.11270</td>\n",
       "      <td>0.07400</td>\n",
       "      <td>...</td>\n",
       "      <td>22.88</td>\n",
       "      <td>27.66</td>\n",
       "      <td>153.20</td>\n",
       "      <td>1606.0</td>\n",
       "      <td>0.1442</td>\n",
       "      <td>0.2576</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.3063</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>84458202</td>\n",
       "      <td>M</td>\n",
       "      <td>13.71</td>\n",
       "      <td>20.83</td>\n",
       "      <td>90.20</td>\n",
       "      <td>577.9</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0.16450</td>\n",
       "      <td>0.09366</td>\n",
       "      <td>0.05985</td>\n",
       "      <td>...</td>\n",
       "      <td>17.06</td>\n",
       "      <td>28.14</td>\n",
       "      <td>110.60</td>\n",
       "      <td>897.0</td>\n",
       "      <td>0.1654</td>\n",
       "      <td>0.3682</td>\n",
       "      <td>0.2678</td>\n",
       "      <td>0.1556</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>0.11510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>844981</td>\n",
       "      <td>M</td>\n",
       "      <td>13.00</td>\n",
       "      <td>21.82</td>\n",
       "      <td>87.50</td>\n",
       "      <td>519.8</td>\n",
       "      <td>0.12730</td>\n",
       "      <td>0.19320</td>\n",
       "      <td>0.18590</td>\n",
       "      <td>0.09353</td>\n",
       "      <td>...</td>\n",
       "      <td>15.49</td>\n",
       "      <td>30.73</td>\n",
       "      <td>106.20</td>\n",
       "      <td>739.3</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.5401</td>\n",
       "      <td>0.5390</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.4378</td>\n",
       "      <td>0.10720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>84501001</td>\n",
       "      <td>M</td>\n",
       "      <td>12.46</td>\n",
       "      <td>24.04</td>\n",
       "      <td>83.97</td>\n",
       "      <td>475.9</td>\n",
       "      <td>0.11860</td>\n",
       "      <td>0.23960</td>\n",
       "      <td>0.22730</td>\n",
       "      <td>0.08543</td>\n",
       "      <td>...</td>\n",
       "      <td>15.09</td>\n",
       "      <td>40.68</td>\n",
       "      <td>97.65</td>\n",
       "      <td>711.4</td>\n",
       "      <td>0.1853</td>\n",
       "      <td>1.0580</td>\n",
       "      <td>1.1050</td>\n",
       "      <td>0.2210</td>\n",
       "      <td>0.4366</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1      2      3       4       5        6        7        8   \\\n",
       "0    842302  M  17.99  10.38  122.80  1001.0  0.11840  0.27760  0.30010   \n",
       "1    842517  M  20.57  17.77  132.90  1326.0  0.08474  0.07864  0.08690   \n",
       "2  84300903  M  19.69  21.25  130.00  1203.0  0.10960  0.15990  0.19740   \n",
       "3  84348301  M  11.42  20.38   77.58   386.1  0.14250  0.28390  0.24140   \n",
       "4  84358402  M  20.29  14.34  135.10  1297.0  0.10030  0.13280  0.19800   \n",
       "5    843786  M  12.45  15.70   82.57   477.1  0.12780  0.17000  0.15780   \n",
       "6    844359  M  18.25  19.98  119.60  1040.0  0.09463  0.10900  0.11270   \n",
       "7  84458202  M  13.71  20.83   90.20   577.9  0.11890  0.16450  0.09366   \n",
       "8    844981  M  13.00  21.82   87.50   519.8  0.12730  0.19320  0.18590   \n",
       "9  84501001  M  12.46  24.04   83.97   475.9  0.11860  0.23960  0.22730   \n",
       "\n",
       "        9   ...     22     23      24      25      26      27      28      29  \\\n",
       "0  0.14710  ...  25.38  17.33  184.60  2019.0  0.1622  0.6656  0.7119  0.2654   \n",
       "1  0.07017  ...  24.99  23.41  158.80  1956.0  0.1238  0.1866  0.2416  0.1860   \n",
       "2  0.12790  ...  23.57  25.53  152.50  1709.0  0.1444  0.4245  0.4504  0.2430   \n",
       "3  0.10520  ...  14.91  26.50   98.87   567.7  0.2098  0.8663  0.6869  0.2575   \n",
       "4  0.10430  ...  22.54  16.67  152.20  1575.0  0.1374  0.2050  0.4000  0.1625   \n",
       "5  0.08089  ...  15.47  23.75  103.40   741.6  0.1791  0.5249  0.5355  0.1741   \n",
       "6  0.07400  ...  22.88  27.66  153.20  1606.0  0.1442  0.2576  0.3784  0.1932   \n",
       "7  0.05985  ...  17.06  28.14  110.60   897.0  0.1654  0.3682  0.2678  0.1556   \n",
       "8  0.09353  ...  15.49  30.73  106.20   739.3  0.1703  0.5401  0.5390  0.2060   \n",
       "9  0.08543  ...  15.09  40.68   97.65   711.4  0.1853  1.0580  1.1050  0.2210   \n",
       "\n",
       "       30       31  \n",
       "0  0.4601  0.11890  \n",
       "1  0.2750  0.08902  \n",
       "2  0.3613  0.08758  \n",
       "3  0.6638  0.17300  \n",
       "4  0.2364  0.07678  \n",
       "5  0.3985  0.12440  \n",
       "6  0.3063  0.08368  \n",
       "7  0.3196  0.11510  \n",
       "8  0.4378  0.10720  \n",
       "9  0.4366  0.20750  \n",
       "\n",
       "[10 rows x 32 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "breast = pd.read_csv(\"http://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\", header=None)\n",
    "print(breast.shape)\n",
    "breast.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FbOTSvdu2hrP"
   },
   "outputs": [],
   "source": [
    "X = breast.drop([0,1], axis=1).values\n",
    "y = breast[1].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13673,
     "status": "ok",
     "timestamp": 1554517385966,
     "user": {
      "displayName": "Chao Tao",
      "photoUrl": "https://lh5.googleusercontent.com/-jFo-3ih4A9w/AAAAAAAAAAI/AAAAAAAAABs/gNwz73yXKSs/s64/photo.jpg",
      "userId": "14116835992842562865"
     },
     "user_tz": 240
    },
    "id": "PfSN4c5GXVXf",
    "outputId": "d7895364-de2d-431f-acfd-dffbc8fb59dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.06673522632219842\n",
      "0.027174786761758857\n",
      "0.043190075723963134\n",
      "0.019241510993765865\n",
      "0.004395604395604396\n",
      "0.006054697806952679\n",
      "0.006054697806952679\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1\n",
      "0.02090060440511415\n",
      "0.026955302212066826\n",
      "0.0\n",
      "0.013987973574945672\n",
      "0.023637115389370262\n",
      "0.0\n",
      "0.013186813186813187\n",
      "2\n",
      "0.04309811093282816\n",
      "0.0159233241710693\n",
      "0.022779182366154463\n",
      "0.0\n",
      "0.02969181319632294\n",
      "0.026373626373626374\n",
      "0.020318928566673694\n",
      "3\n",
      "0.11462632509543107\n",
      "0.006054697806952679\n",
      "0.007132115379860508\n",
      "0.02255969781646243\n",
      "0.008791208791208791\n",
      "0.04951500002852925\n",
      "0.004395604395604396\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "4\n",
      "0.14423220418778093\n",
      "0.02112008895480618\n",
      "0.008791208791208791\n",
      "0.08351648351648351\n",
      "0.0\n",
      "0.059603110942337896\n",
      "0.037936538305142944\n"
     ]
    }
   ],
   "source": [
    "rf = randomforest()\n",
    "#Change 4 and 6\n",
    "rf_model = rf.rf(X_train, y_train, list(range(1, 31)), 40, 500, 7, maxlevel=3)\n",
    "out = rf.rfclass(rf_model, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compared to Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this one, I was tasked with changing the number of trees (nTrees), and number of features (nFeatures) being used in the implementation. I have changed them to what I believe is also an efficient accuracy score, considering the model. Compared to project 2 (which ran on the CIFAR-10 data set) it did not do as well. I was able to get a 93% accuracy rating one of my project 2 implementations. I did not achieve that here. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "maketree.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
